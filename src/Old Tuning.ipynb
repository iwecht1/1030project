{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bced763",
   "metadata": {},
   "source": [
    "<h1><center>Solving for Happiness - Machine Learning Model Tuning</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c9d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to be used throughout project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import math\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a70093d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/X1_other.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front = '../data/'\n",
    "back_other = '_other.csv'\n",
    "back_test = '_test.csv'\n",
    "tot = front + 'X1' + back\n",
    "tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f42360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to grab right files from directory \n",
    "X = ['X1','X2','X3','X4','X5']\n",
    "y = ['y1','y2','y3','y4','y5']\n",
    "\n",
    "# lists to save all X_other, X_test, y_other, and y_test across all lags \n",
    "X_other_list = []\n",
    "X_test_list = []\n",
    "y_other_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# save all lagged dataframes to list\n",
    "for i in range(5):\n",
    "    \n",
    "    # import laged dataframe \n",
    "    X_other = pd.read_csv(front + X[i] + back_other)\n",
    "    X_other.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "    X_test = pd.read_csv(front + X[i] + back_test)\n",
    "    X_test.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "    y_other = pd.read_csv(front + y[i] + back_other)\n",
    "    y_other.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "    y_test = pd.read_csv(front + y[i] + back_test)\n",
    "    y_test.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "    # convert dataframes to series \n",
    "    y_test = y_test['Target Score']\n",
    "    y_other = y_other['Target Score']\n",
    "\n",
    "    X_other_list.append(X_other)\n",
    "    X_test_list.append(X_test)\n",
    "    y_other_list.append(y_other)\n",
    "    y_test_list.append(y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29e1e8",
   "metadata": {},
   "source": [
    "<h2>Baseline Regression Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d967d",
   "metadata": {},
   "source": [
    "The below analysis examines the baseline RMSE metric for a constant regression model. This metric is calculated by predicting the average value of the target variable for each of the datapoints. This baseline will be used to judge each of the machine learning algorithms below. A 'good' model should produce a RMSE score that is at least lower than the baseline metric calculated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19c9c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE Score: 1.042\n",
      "Baseline R2 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# create true series for analysis\n",
    "y_true = pd.concat([y_other,y_test])\n",
    "\n",
    "# create mean series\n",
    "pred_avg = y_true.mean()\n",
    "\n",
    "avg_list = np.ones(len(y_true))*pred_avg\n",
    "y_avg = pd.Series(avg_list)\n",
    "\n",
    "# calculate the baseline RMSE metric\n",
    "mse_base = mean_squared_error(y_true,y_avg)\n",
    "rmse_base = math.sqrt(mse_base)\n",
    "r2_base = r2_score(y_true,y_avg)\n",
    "\n",
    "print('Baseline RMSE Score: {:.4}'.format(rmse_base))\n",
    "print('Baseline R2 Score: {:.4}'.format(r2_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc1ea1",
   "metadata": {},
   "source": [
    "<h2> General Machine Learning Algorithm </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861b11c",
   "metadata": {},
   "source": [
    "The below code creates a PredefinedSplit object, which will be used to maintain the data integrity of the train and validation sets during the GrindSearchCV training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d2795fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits to be created by ps: 1\n",
      "Number of splits to be created by ps: 1\n",
      "Number of splits to be created by ps: 1\n",
      "Number of splits to be created by ps: 1\n",
      "Number of splits to be created by ps: 1\n"
     ]
    }
   ],
   "source": [
    "# import predefinedsplit to maintain data integrity during cv\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# all predefined splits \n",
    "all_ps = []\n",
    "\n",
    "# indexes for the different train sets \n",
    "ti = [470,423,470,517,564]\n",
    "\n",
    "# The indices which have zero or positive values, will be kept in validation during the grid split\n",
    "val_indices = np.full((94,), 0, dtype=int)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    # The indices which have the value -1 will be kept in train during the grid split \n",
    "    train_indices = np.full((ti[i],), -1, dtype=int)\n",
    "\n",
    "    test_fold = np.append(train_indices, val_indices)\n",
    "\n",
    "    # create a predefinedsplit instance that has the indexes of the train and val sets \n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    all_ps.append(ps)\n",
    "\n",
    "# Check how many splits will be done, based on test_fold\n",
    "for i in range(5):\n",
    "    print('Number of splits to be created by ps:',all_ps[i].get_n_splits())\n",
    "\n",
    "# print the indexes of the train and validation sets \n",
    "# for train_index, test_index in ps.split():\n",
    "#     print(\"TRAIN Datapoints:\\n\", train_index)\n",
    "#     print(\"Validation Datapoints:\\n\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8478",
   "metadata": {},
   "source": [
    "The below code creates a preprocessor, which will be passed to GrindSearchCV in order to preprocess the dataset before completing the training and validation of our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c025eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for standardscaling preprocessing\n",
    "num_ftrs = ['Target Year', \n",
    "            'Population lag 3 years','Population lag 2 years', 'Population lag 1 year', 'Population Current',\n",
    "            'Life Ex lag 3 years', 'Life Ex lag 2 years', 'Life Ex lag 1 year','Life Ex Current', \n",
    "#             'GDP lag 3 years', 'GDP lag 2 years','GDP lag 1 year', 'GDP Current', \n",
    "            'GDP_cap lag 3 years','GDP_cap lag 2 years', 'GDP_cap lag 1 year', 'GDP_cap Current',\n",
    "            'CO2 lag 3 years', 'CO2 lag 2 years', 'CO2 lag 1 year', 'CO2 Current',\n",
    "            'Happy lag 3 years', 'Happy lag 2 years', 'Happy lag 1 year']\n",
    "\n",
    "# feature for categorical preprocessing\n",
    "cat_ftrs = ['Country']\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# create a preprocessor from the collected the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfdb6e",
   "metadata": {},
   "source": [
    "The below function is a general purpose algorithm that will be used throughout this notebook to train and test various machine learning models. The function takes the train and validation sets (X_other, y_other), the test sets, an machine learning algorithm, and the parameter grid to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322e2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function to find best models and test scores \n",
    "def MLpipe_RMSE(X_other,X_test,y_other,y_test,preprocessor,ps,ml_algo,para_grid,bestModels,bestScores,bestPara):\n",
    "\n",
    "    # create pipe for modeling algorithm\n",
    "    pipe = make_pipeline(preprocessor,ml_algo)\n",
    "\n",
    "    # create grid to loop through hyperparameter combinations and collect results\n",
    "    grid = GridSearchCV(pipe,param_grid=para_grid,scoring='neg_root_mean_squared_error',cv=ps,\n",
    "                        return_train_score=False,n_jobs=-1,verbose=False)\n",
    "\n",
    "    # fit the model onto other, this will be split between train and val based on ps \n",
    "    grid.fit(X_other,y_other)\n",
    "\n",
    "    # save the hyperparameters that made for the best model         \n",
    "    bestPara.append(grid.best_params_)\n",
    "\n",
    "    # save the model\n",
    "    bestModels.append(grid)\n",
    "\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = bestModels[-1].predict(X_test)\n",
    "    bestScores.append(math.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "#     bestScores.append(r2_score(y_test,y_test_pred))\n",
    "\n",
    "    return bestModels,bestScores, bestPara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20d51358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of random states to test different models on \n",
    "random_states = [0,5,15,37,42,68,97,100,234,583]\n",
    "\n",
    "# list to collect all models scores\n",
    "model_scores = []\n",
    "std_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cef308",
   "metadata": {},
   "source": [
    "<h2>Linear Regression Modeling - Lasso</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f41b0785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.395e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score for Lasso (l1 Regularization): 0.24906540521327092\n",
      "Standard deviation of test scores for l1: 0.0\n",
      "\n",
      "Best Parameters:\n",
      "Alpha: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+01, tolerance: 4.251e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#import lasso regression object \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# lists to hold best models and test scores      \n",
    "l1_bestModels = []\n",
    "l1_bestScores = []\n",
    "l1_bestPara = []\n",
    "\n",
    "# hyperparameters to tune\n",
    "param_grid = {\n",
    "              'lasso__alpha': np.logspace(-30,10,21) # the alpha value for regularization\n",
    "              } \n",
    "\n",
    "# train and test models for each random state\n",
    "# for state in random_states:\n",
    "    # initialize lasso model object\n",
    "l1_mod = Lasso(max_iter=10000,random_state=0)\n",
    "\n",
    "# call function and save scores and models\n",
    "l1_bestModels, l1_bestScores, l1_bestPara = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "                                        preprocessor,ps,l1_mod,param_grid,\n",
    "                                        l1_bestModels,l1_bestScores,l1_bestPara)\n",
    "\n",
    "# print results\n",
    "print('Test score for Lasso (l1 Regularization):',np.average(l1_bestScores))\n",
    "print('Standard deviation of test scores for l1:', np.std(l1_bestScores))\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "for k,v in l1_bestPara[0].items():\n",
    "    print('Alpha:',v)\n",
    "    \n",
    "model_scores.append(np.average(l1_bestScores))\n",
    "std_scores.append(np.std(l1_bestScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce612f0",
   "metadata": {},
   "source": [
    "<h2>Linear Regression Modeling - Ridge</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fdedc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create l2 regularization model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# lists to hold best models and test scores      \n",
    "l2_bestModels = []\n",
    "l2_bestScores = []\n",
    "l2_bestPara = []\n",
    "\n",
    "# hyperparameters to tune\n",
    "param_grid = {\n",
    "              'ridge__alpha': np.logspace(-7,7,21) # the alpha value for regularization\n",
    "              } \n",
    "\n",
    "# initialize lasso model object\n",
    "l2_mod = Ridge(max_iter=10000000,random_state=0)\n",
    "\n",
    "# call function and save scores and models\n",
    "l2_bestModels, l2_bestScores, l2_bestPara = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "                                        preprocessor,ps,l2_mod,param_grid,\n",
    "                                        l2_bestModels,l2_bestScores,l2_bestPara)\n",
    "\n",
    "\n",
    "# print results\n",
    "print('Average test score for l1:',np.average(l2_bestScores))\n",
    "print('Standard deviation of test scores for l1:', np.std(l2_bestScores))\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "for k,v in l2_bestPara[0].items():\n",
    "    print('Alpha:',v)\n",
    "\n",
    "model_scores.append(np.average(l2_bestScores))\n",
    "std_scores.append(np.std(l2_bestScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3e338",
   "metadata": {},
   "source": [
    "<h2>Linear Regression Modeling - ElasticNet</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9a917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # create elastic net regularization model\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# # lists to hold best models and test scores      \n",
    "# en_bestModels = []\n",
    "# en_bestScores = []\n",
    "# en_bestPara = []\n",
    "\n",
    "# # initialize model algo\n",
    "# en_mod = ElasticNet(max_iter=1000000,random_state=12,selection='random')\n",
    "\n",
    "# # hyperparameters to tune\n",
    "# param_grid = {\n",
    "#               'elasticnet__alpha': np.logspace(-7,7,21), # the alpha value for regularization\n",
    "#               'elasticnet__l1_ratio': np.linspace(0.01,1,21) # weight for lasso and ridge \n",
    "#               } \n",
    "\n",
    "# # call function to generate models and store scores\n",
    "# en_bestModels, en_bestScores, en_bestParams = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "#                                         preprocessor,ps,en_mod,param_grid,\n",
    "#                                         en_bestModels,en_bestScores,en_bestPara)\n",
    "\n",
    "# # print mean and std of test scores\n",
    "# print('Average test score for elasticnet:',np.average(en_bestScores))\n",
    "# print('Standard deviation of test scores for elasticnet:', np.std(en_bestScores))\n",
    "\n",
    "# bp = pd.DataFrame(en_bestParams)\n",
    "# print('\\nBest Parameters:')   \n",
    "# bp\n",
    "\n",
    "# model_scores.append(np.average(en_bestScores))\n",
    "# std_scores.append(np.std(en_bestScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd505ea6",
   "metadata": {},
   "source": [
    "<h2>Random Forest Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720e875",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# lists to hold best models and test scores      \n",
    "rf_bestModels = []\n",
    "rf_bestScores = []\n",
    "rf_bestPara = []\n",
    "\n",
    "# hyperparameters to tune\n",
    "param_grid = {\n",
    "              'randomforestregressor__max_depth': [1, 3, 5, 7, 10,15,20], # the max_depth of each tree\n",
    "              'randomforestregressor__max_features': [0.15, 0.25, 0.5,0.75,1.0], # linearly spaced between 0.5 and 1\n",
    "              }  \n",
    "\n",
    "# train and test models for each random state\n",
    "for state in random_states:\n",
    "    # initialize model algo\n",
    "    rf_mod = RandomForestRegressor(random_state=state)\n",
    "\n",
    "    # call function and save scores and models\n",
    "    rf_bestModels, rf_bestScores, rf_bestPara = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "                                            preprocessor,ps,rf_mod,param_grid,\n",
    "                                            rf_bestModels,rf_bestScores,rf_bestPara)\n",
    "\n",
    "\n",
    "# print results\n",
    "print('Average test score for random forest:',np.average(rf_bestScores))\n",
    "print('Standard deviation of test scores for random forest:', np.std(rf_bestScores))\n",
    "model_scores.append(np.average(rf_bestScores))\n",
    "std_scores.append(np.std(rf_bestScores))\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "bp = pd.DataFrame(rf_bestPara)   \n",
    "bp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c960ba",
   "metadata": {},
   "source": [
    "<h2>Support Vector Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ecbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SVR model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# lists to hold best models and test scores      \n",
    "svr_bestModels = []\n",
    "svr_bestScores = []\n",
    "svr_bestPara = []\n",
    "\n",
    "# initialize model algo\n",
    "svr_mod = SVR()\n",
    "\n",
    "# hyperparameters to tune\n",
    "param_grid = {\n",
    "              'svr__gamma': np.logspace(-8,0,15),\n",
    "              'svr__C': np.logspace(-1,3,15)\n",
    "              }  \n",
    "\n",
    "# call function to generate models and store scores\n",
    "svr_bestModels, svr_bestScores, svr_bestPara = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "                                        preprocessor,ps,svr_mod,param_grid,\n",
    "                                        svr_bestModels,svr_bestScores,svr_bestPara)\n",
    "\n",
    "# print mean and std of test scores\n",
    "print('Average test score for support vector regressor:',np.average(svr_bestScores))\n",
    "print('Standard deviation of test scores for support vector regressor:', np.std(svr_bestScores))\n",
    "model_scores.append(np.average(svr_bestScores))\n",
    "std_scores.append(np.std(svr_bestScores))\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "bp = pd.DataFrame(svr_bestPara)  \n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa482e",
   "metadata": {},
   "source": [
    "<h2>K-Nearest Neighbor Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4886752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Nearest Neighbor model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# lists to hold best models and test scores      \n",
    "k_bestModels = []\n",
    "k_bestScores = []\n",
    "k_bestPara = []\n",
    "\n",
    "\n",
    "# initialize model algo\n",
    "k_mod = KNeighborsRegressor()\n",
    "\n",
    "# hyperparameters to tune\n",
    "param_grid = {\n",
    "              'kneighborsregressor__n_neighbors': np.linspace(1,10,10,dtype=int), # amount of neighbors to use \n",
    "              'kneighborsregressor__weights': ['uniform','distance'] # type of weight to consider\n",
    "              }  \n",
    "\n",
    "# call function to generate models and store scores\n",
    "k_bestModels, k_bestScores, k_bestPara = MLpipe_RMSE(X_other,X_test,y_other,y_test,\n",
    "                                        preprocessor,ps,k_mod,param_grid,\n",
    "                                        k_bestModels,k_bestScores,k_bestPara)\n",
    "\n",
    "# print mean and std of test scores\n",
    "print('Average test score for k-nearest neighbor:',np.average(k_bestScores))\n",
    "print('Standard deviation of test scores for k-nearest neighbor:', np.std(k_bestScores))\n",
    "model_scores.append(np.average(k_bestScores))\n",
    "std_scores.append(np.std(k_bestScores))\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "bp = pd.DataFrame(k_bestPara)  \n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c7da7",
   "metadata": {},
   "source": [
    "<h2>XGBoost Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4425b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# split the train and val sets \n",
    "X_train = X_other.iloc[:376]\n",
    "y_train = y_other.iloc[:376]\n",
    "X_val = X_other.iloc[376:]\n",
    "y_val = y_other.iloc[376:]\n",
    "\n",
    "# fit the preprocessor to the train data and transform train, val, and test    \n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_val_prep = preprocessor.transform(X_val)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "# random states to tune with\n",
    "seeds = [5,15,25,35,45]\n",
    "alphas = [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "\n",
    "# parameter grid for model tuning     \n",
    "grid = {\n",
    "    \"reg_alpha\": alphas\n",
    "   }\n",
    "\n",
    "# list to save best test scores, all test sets, and best models  \n",
    "test_scores = []\n",
    "best_models = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # list to save scores per random state     \n",
    "    val_scores = []\n",
    "    all_models = []\n",
    "    all_test_scores = []\n",
    "    \n",
    "    # loop through and tune alphas\n",
    "    for j in range(len(alphas)):\n",
    "        # initialize and set the alpha for xgboost         \n",
    "        XGB = xgboost.XGBRegressor(random_state=seed)\n",
    "        XGB.set_params(**ParameterGrid(grid)[j])\n",
    "\n",
    "        # fit model to the train data, evaluate on the validation set         \n",
    "        XGB.fit(X_train_prep,y_train,early_stopping_rounds=50,eval_set=[(X_val_prep, y_val)],\n",
    "                verbose=False)\n",
    "\n",
    "        # predict the validation and test variables          \n",
    "        y_val_pred = XGB.predict(X_val_prep)\n",
    "        y_test_pred = XGB.predict(X_test_prep)\n",
    "\n",
    "        # add validation and test scores to list          \n",
    "        val_scores.append(np.sqrt(mean_squared_error(y_val,y_val_pred)))\n",
    "        all_test_scores.append(np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "        \n",
    "        # add all models to list, will tune down to best_models outside hyper loop \n",
    "        all_models.append(XGB)\n",
    "\n",
    "    # find index of best validation score \n",
    "    index = np.argmax(val_scores)\n",
    "    test_scores.append(all_test_scores[index])\n",
    "    best_models.append(all_models[index])\n",
    "\n",
    "    # print(test_scores)\n",
    "    # print(best_models)\n",
    "    \n",
    "model_scores.append(np.average(test_scores))\n",
    "std_scores.append(np.std(test_scores))\n",
    "\n",
    "print('Mean of the test scores for XGBoost:',np.average(test_scores))\n",
    "print('Standard deviation of the test scores for XGBoost:',np.std(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2675498",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Lasso','Ridge','Random Forest','SVR','Nearest Neighbor','XGBoost']\n",
    "final_out = {'Model':models,'Mean Score':model_scores,'Std Score':std_scores}\n",
    "fdf = pd.DataFrame(final_out,index=[1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "df = pd.DataFrame(rslt[['Model','Mean Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# add the plot\n",
    "sns.barplot(x='Model', y='Mean Score', data=df, capsize=0.2, ax=ax)\n",
    "\n",
    "# add the annotation\n",
    "ax.bar_label(ax.containers[-1], fmt='Mean:\\n%.3f', label_type='center')\n",
    "\n",
    "# lable plot\n",
    "plt.title('Comparison of Model Test Scores',weight='bold')\n",
    "plt.xlabel('Model',weight='bold')\n",
    "plt.ylabel('Test Score (RMSE)',weight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
